{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReadMe:**\n",
    "\n",
    "Please run the code in the notebook sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk7NAntc9qm8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdHE0KWL992L"
   },
   "source": [
    "# 1. Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buSzz6Sk9qnB"
   },
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    # TensorDataset with support of transforms.\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "def get_loader(train_X, val_X, test_X, train_y, val_y, test_y, train_tfms, valid_tfms, batch_size =128, num_workers = 1):\n",
    "    # This function is used to convert the dataset into DataLoaders.  \n",
    "    # The training data's, validation data's and testing data's corresponding DataLoader objects are returned.\n",
    "    \n",
    "    # The training dataset\n",
    "    train_data = CustomTensorDataset((train_X, train_y), train_tfms)\n",
    "    # The vailidation dataset\n",
    "    val_data = CustomTensorDataset((val_X, val_y), valid_tfms)\n",
    "    # The testing dataset\n",
    "    test_data = CustomTensorDataset((test_X, test_y), valid_tfms) \n",
    "\n",
    "    # The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch, do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    # Return the three DataLodaer\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogCAm2H59qnD"
   },
   "source": [
    "# 2. Neural network structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_wljVa_V3y"
   },
   "source": [
    "## 2.1 Fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh9_8qvm_aAy"
   },
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_dim = 784):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.linear_layer_1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.hidden_layer_1 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.drop2 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.hidden_layer_2 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.hidden_layer_3 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.drop4 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.hidden_layer_4 = nn.Linear(128, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.drop5 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.hidden_layer_5 = nn.Linear(64, 32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.drop6 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.output_layer = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define a fully connected network with 5 hidden layer.\n",
    "        out = torch.flatten(x, 1)\n",
    "        out = self.drop1(self.bn1(F.relu(self.linear_layer_1(out))))\n",
    "        out = self.drop2(self.bn2(F.relu(self.hidden_layer_1(out))))\n",
    "        out = self.drop3(self.bn3(F.relu(self.hidden_layer_2(out))))\n",
    "        out = self.drop4(self.bn4(F.relu(self.hidden_layer_3(out))))\n",
    "        out = self.drop5(self.bn5(F.relu(self.hidden_layer_4(out))))\n",
    "        out = self.drop6(self.bn6(F.relu(self.hidden_layer_5(out))))\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        probas = F.softmax(out, dim=1)\n",
    "\n",
    "        # Return the logit and the probability\n",
    "        return out, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwYO0PLc_ddO"
   },
   "source": [
    "## 2.2 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n__82b14_fJ3"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, grayscale=True):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Handle different input channel\n",
    "        if grayscale:\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Adjust different dimensions for gray scale and RGB data\n",
    "        if grayscale:\n",
    "            self.fc1 = nn.Linear(in_features=128*4*4, out_features=600)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_features=128*5*5, out_features=600)\n",
    "        self.drop1 = nn.Dropout2d(0.1)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.drop2 = nn.Dropout2d(0.1)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=120)\n",
    "        self.drop3 = nn.Dropout2d(0.1)\n",
    "        self.fc4 = nn.Linear(in_features=120, out_features=120)\n",
    "        self.fc5 = nn.Linear(in_features=120, out_features=3)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.drop1(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.drop2(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.drop3(out)\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "        probas = F.softmax(out, dim=1)\n",
    "        \n",
    "        # Return the logit and the probability\n",
    "        return out, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUvRklih_OsS"
   },
   "source": [
    "## 2.3 Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58RUdXQMKnyP"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # The basic block for resnet\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# This is the block used for ResNet 50 and others\n",
    "# We have only used ResNet 18 for running the experiment for saving the computational time\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=3, grayscale=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        # Handle different input channel for gray scale or RGB\n",
    "        if grayscale:\n",
    "            self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        probas = F.softmax(out, dim=1)\n",
    "        # Return the logit and the probability\n",
    "        return out, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSmFqy_V_wxz"
   },
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7ptasP79qnE"
   },
   "source": [
    "## 3.1 Helper functions for training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5X7is4HQz-P"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_noisy(probas, target):\n",
    "    return accuracy_score(torch.max(probas, 1)[1].cpu(), target.cpu())\n",
    "\n",
    "# A helper function which is used to record the experiment results.\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "# A helper function to calculate the testing Top-1 accuracy\n",
    "def compute_accuracy(model, data_loader, Ta):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):           \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "\n",
    "        # Convert to column vector\n",
    "        probas = probas.T\n",
    "\n",
    "        # Multiply with T^{-1}\n",
    "        probas = torch.matmul(torch.inverse(Ta), probas).T\n",
    "\n",
    "        # Convert the one hot embedding to predicted label with highest probability\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "\n",
    "        # Calculate the Top-1 accuracy\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCyqIslJCbRh"
   },
   "outputs": [],
   "source": [
    "# When all random seeds are fixed, the python runtime environment becomes deterministic.\n",
    "def seed_torch(seed=1029):\n",
    "    r\"\"\"Fix all random seeds for repeating the expriement result.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # If multi-GPUs are used. \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RhXInOFAbUl"
   },
   "source": [
    "## 3.2 Functions for evaluating transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vReZY_2XAaqn"
   },
   "outputs": [],
   "source": [
    "# The conversion function for one hot embedding\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    r\"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRhj4D26A97A"
   },
   "outputs": [],
   "source": [
    "# The evaluation function for the transition matrix by the anchor points method\n",
    "def eval_tran_mat(model, train_loader):\n",
    "    r\"\"\"caluate the transition matrix \n",
    "    by using the prediction of a the trained classifier (the anchor points assumption). \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pos_condition_p = []\n",
    "    for step, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        labels = targets.numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, probas = model(data)\n",
    "\n",
    "        one_hot_labels = one_hot_embedding(targets,3).numpy()\n",
    "        probas = probas.cpu().data.numpy()\n",
    "        pos_condition_p += probas.tolist() \n",
    "        \n",
    "    pos_condition_p = np.array(pos_condition_p)\n",
    "    col1 = pos_condition_p[np.argmax(pos_condition_p[:, 0])].reshape(-1,1)\n",
    "    col2 = pos_condition_p[np.argmax(pos_condition_p[:, 1])].reshape(-1,1)\n",
    "    col3 = pos_condition_p[np.argmax(pos_condition_p[:, 2])].reshape(-1,1)\n",
    "\n",
    "    return np.concatenate((col1, col2, col3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZayDvN5KnyQ"
   },
   "outputs": [],
   "source": [
    "# The evaluation function for the T^A matrix by the dual-T estimator\n",
    "def eval_tran_mat_TA(model, train_loader):\n",
    "    r\"\"\"caluate the the transition matrix\n",
    "    by using the prediction of a the trained classifier (the dual-T estimator). \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_data_tol = []\n",
    "    for step, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        labels = targets.numpy().reshape(-1,1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, probas = model(data)\n",
    "\n",
    "        prect_y = np.array(torch.max(probas, 1)[1].cpu()).reshape(-1,1)\n",
    "        y_data = np.concatenate((prect_y, labels), axis=1).tolist()\n",
    "        y_data_tol += y_data\n",
    "        \n",
    "    result = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            result[i, j] = y_data_tol.count([i,j])\n",
    "\n",
    "    Ta = result.copy()\n",
    "    for i in range(3):\n",
    "        if np.sum(result[i]) != 0:\n",
    "            Ta[i] /= np.sum(result[i])\n",
    "\n",
    "    return Ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aPxPLfD9qnE"
   },
   "source": [
    "## 3.3 Function for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APpENDwJ9qnF"
   },
   "outputs": [],
   "source": [
    "# The training, validating and testing function\n",
    "def train_val_test(model, train_loader, val_loader, test_loader, optimizer, Ta, num_epoch = 4):\n",
    "    # Training process\n",
    "    for epoch in range(num_epoch):\n",
    "        # Restart the average meter\n",
    "        ave_meter = AverageMeter()\n",
    "        \n",
    "        model.train()\n",
    "        train_index = 0\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "                \n",
    "            # Forward and back propagation\n",
    "            logits, probas = model(features)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            cost.backward()\n",
    "            \n",
    "            # Updata the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the training accuracy\n",
    "            acc = compute_accuracy_noisy(probas, targets)\n",
    "            ave_meter.update(acc, targets.size(0))\n",
    "            \n",
    "            # As we are not required to present the training accuracy, we abandon this information\n",
    "            \n",
    "\n",
    "    # Validation process\n",
    "    # Restart the average meter\n",
    "    ave_meter = AverageMeter()\n",
    "    for batch_idx, (features, targets) in enumerate(val_loader): \n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward prediction\n",
    "        logits, probas = model(features)\n",
    "\n",
    "        # Store the validation accuracy\n",
    "        acc = compute_accuracy_noisy(probas, targets)\n",
    "        ave_meter.update(acc, targets.size(0))\n",
    "\n",
    "    # Get the avaerage validation accuracy\n",
    "    average_acc_val = ave_meter.avg\n",
    "\n",
    "    # The transition matrix estmated by anchor points method\n",
    "    estimated_TB = eval_tran_mat(model, train_loader)\n",
    "\n",
    "    # The T^A matrix estmated by dual-T method\n",
    "    estimated_TA = eval_tran_mat_TA(model, train_loader)\n",
    "\n",
    "    # To save memory during inference\n",
    "    with torch.set_grad_enabled(False): \n",
    "        # Compute the testing accuracy using the provided transition matrix\n",
    "        average_acc_test = compute_accuracy(model, test_loader, Ta)\n",
    "\n",
    "    return average_acc_val, average_acc_test, estimated_TB, np.matmul(estimated_TA, estimated_TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHyH9zqMKnyR"
   },
   "outputs": [],
   "source": [
    "# As the CIFIAR dataset has not known ground truth transition matrix, we use this function for evaluting the two estimation methods.\n",
    "def train_val_test_CIFAR(model, train_loader, val_loader, test_loader, optimizer, Ta, Ta_dual, num_epoch = 4):\n",
    "    # Ta is the transition matrix estimated by anchor points method\n",
    "    # Ta_dual is the transition matrix estimated by dual-T method\n",
    "    for epoch in range(num_epoch):\n",
    "        ave_meter = AverageMeter()\n",
    "        \n",
    "        model.train()\n",
    "        train_index = 0\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "                \n",
    "            # Forward and back propagation\n",
    "            logits, probas = model(features)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            cost.backward()\n",
    "            \n",
    "            # Updata the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the training accuracy\n",
    "            acc = compute_accuracy_noisy(probas, targets)\n",
    "            ave_meter.update(acc, targets.size(0))\n",
    "\n",
    "            # As we are not required to present the training accuracy, we abandon this information\n",
    "\n",
    "\n",
    "    # Validation process\n",
    "    # Restart the average meter\n",
    "    ave_meter = AverageMeter()\n",
    "    for batch_idx, (features, targets) in enumerate(val_loader): \n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward prediction\n",
    "        logits, probas = model(features)\n",
    "\n",
    "        # Store the validation accuracy\n",
    "        acc = compute_accuracy_noisy(probas, targets)\n",
    "        ave_meter.update(acc, targets.size(0))\n",
    "\n",
    "    # Get the avaerage validation accuracy\n",
    "    average_acc_val = ave_meter.avg\n",
    "\n",
    "    # The transition matrix estmated by anchor points method\n",
    "    estimated_TB = eval_tran_mat(model, train_loader)\n",
    "\n",
    "    # The T^A matrix estmated by dual-T method    \n",
    "    estimated_TA = eval_tran_mat_TA(model, train_loader)\n",
    "\n",
    "    # To save memory during inference\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Compute the testing accuracy using the estimated transition matrix (anchor points method)\n",
    "        average_acc_test = compute_accuracy(model, test_loader, Ta)\n",
    "\n",
    "        # Compute the testing accuracy using the estimated transition matrix (dual-T method)\n",
    "        average_acc_dual_test = compute_accuracy(model, test_loader, Ta_dual)\n",
    "\n",
    "    return average_acc_val, average_acc_test, average_acc_dual_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4LBOpXIG5iv"
   },
   "source": [
    "# 4. Experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okl01t4UG8zw"
   },
   "source": [
    "## 4.1 FashionMINIST0.5.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLWHiyvqHIV2"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = np.load(\"./data/FashionMNIST0.5.npz\")\n",
    "# Convert its shape\n",
    "Xtr_val = dataset['Xtr'].reshape([18000, 1, 28, 28])\n",
    "Str_val = dataset['Str']\n",
    "# Convert its shape\n",
    "Xts = dataset['Xts'].reshape([3000, 1, 28, 28])\n",
    "Yts = dataset['Yts']\n",
    "\n",
    "# The transition matrix provided\n",
    "Ta = torch.tensor([[0.5,0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]).to(device)\n",
    "\n",
    "# Lists to store the testing accuracy\n",
    "test_acc_FNN = []\n",
    "test_acc_CNN = []\n",
    "test_acc_Resnet = []\n",
    "\n",
    "# The optimal model with highest validation performance to be stored\n",
    "best_model = None\n",
    "best_T_estimated = 0\n",
    "best_T_dual_estimated = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "# Loop for 10 random splits\n",
    "for i in range(10):\n",
    "    # Random seeds\n",
    "    seed_torch(2 ** i)\n",
    "\n",
    "    # Splitting the training and validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Xtr_val, Str_val, test_size=0.2)\n",
    "\n",
    "    # Convert the dataset to tensor\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(Xts, dtype=torch.float32)\n",
    "\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test = torch.tensor(Yts, dtype=torch.long)\n",
    "\n",
    "    # For normalization\n",
    "    stats = ((0.1307,), (0.3081,))\n",
    "\n",
    "\n",
    "    # For training transformation\n",
    "    train_tfms = tt.Compose([tt.ToPILImage(),\n",
    "                            tt.RandomHorizontalFlip(), \n",
    "                            tt.ToTensor(), \n",
    "                            tt.Normalize(*stats,inplace=True)])\n",
    "    # For validation and testing transformation  \n",
    "    valid_tfms = tt.Compose([tt.ToPILImage(), tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "    # Get the dataloader\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train, X_val, X_test, y_train, y_val, y_test, train_tfms, valid_tfms, batch_size = 100, num_workers = 0)\n",
    "\n",
    "    # FNN model\n",
    "    FNN_model = FCNet().to(device)\n",
    "    FNN_optimizer = torch.optim.Adam(FNN_model.parameters(), lr=0.001)\n",
    "    FNN_avg_acc_val, FNN_avg_acc_test, FNN_estimated_T, FNN_estimated_T_dual = train_val_test(FNN_model, train_loader, val_loader, test_loader, FNN_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_FNN.append(float(FNN_avg_acc_test.cpu()))\n",
    "\n",
    "    if FNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = FNN_avg_acc_val\n",
    "        best_model = FNN_model\n",
    "        best_T_estimated = FNN_estimated_T\n",
    "        best_T_dual_estimated = FNN_estimated_T_dual\n",
    "\n",
    "    # CNN model\n",
    "    CNN_model = CNN(grayscale=True).to(device)\n",
    "    CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "    CNN_avg_acc_val, CNN_avg_acc_test, CNN_estimated_T, CNN_estimated_T_dual = train_val_test(CNN_model, train_loader, val_loader, test_loader, CNN_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_CNN.append(float(CNN_avg_acc_test.cpu()))\n",
    "\n",
    "    if CNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = CNN_avg_acc_val\n",
    "        best_model = CNN_model\n",
    "        best_T_estimated = CNN_estimated_T\n",
    "        best_T_dual_estimated = CNN_estimated_T_dual\n",
    " \n",
    "    # ResNet model\n",
    "    Resnet_model = ResNet(block=BasicBlock, num_blocks=[2, 2, 2, 2], num_classes=3, grayscale=True).to(device)\n",
    "    Resnet_optimizer = torch.optim.Adam(Resnet_model.parameters(), lr=0.001)\n",
    "    Resnet_avg_acc_val, Resnet_avg_acc_test, Resnet_estimated_T, Resnet_estimated_T_dual = train_val_test(Resnet_model, train_loader, val_loader, test_loader, Resnet_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_Resnet.append(float(Resnet_avg_acc_test.cpu()))\n",
    "\n",
    "    if Resnet_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = Resnet_avg_acc_val\n",
    "        best_model = Resnet_model\n",
    "        best_T_estimated = Resnet_estimated_T\n",
    "        best_T_dual_estimated = Resnet_estimated_T_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3j3E3tI2JQdJ",
    "outputId": "a8744436-6c49-4f78-8810-27e6faa76ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average testing accuracy FNN is 0.8435, and the std is 0.0195\n",
      "The average testing accuracy CNN is 0.9332, and the std is 0.0080\n",
      "The average testing accuracy ResNet is 0.8699, and the std is 0.0454\n"
     ]
    }
   ],
   "source": [
    "test_acc_FNN = np.array(test_acc_FNN)\n",
    "print(f'The average testing accuracy FNN is {test_acc_FNN.mean():.4f}, and the std is {test_acc_FNN.std():.4f}' )\n",
    "\n",
    "test_acc_CNN = np.array(test_acc_CNN)\n",
    "print(f'The average testing accuracy CNN is {test_acc_CNN.mean():.4f}, and the std is {test_acc_CNN.std():.4f}' )\n",
    "\n",
    "test_acc_Resnet = np.array(test_acc_Resnet)\n",
    "print(f'The average testing accuracy ResNet is {test_acc_Resnet.mean():.4f}, and the std is {test_acc_Resnet.std():.4f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLX5_tR3KnyS",
    "outputId": "8d41df70-4bbe-4aa3-ee0f-a0c21461e0b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by loss correction is:\n",
      "[[0.66529697 0.15440804 0.21152596]\n",
      " [0.20973    0.54397291 0.11993556]\n",
      " [0.12497307 0.30161905 0.66853851]]\n",
      "The estimation error in transition matrix (mse) by loss correction is:\n",
      "0.009750985322292405\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by loss correction is:')\n",
    "print(best_T_estimated)\n",
    "\n",
    "# The provided transition matrix\n",
    "T_true = np.array([[0.5,0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])\n",
    "\n",
    "print(\"The estimation error in transition matrix (mse) by loss correction is:\")\n",
    "print((np.square(T_true - best_T_estimated)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHrNyGaSKnyS",
    "outputId": "a6d9be15-1142-4dfb-ef81-197b352cd0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by dual T estimator is:\n",
      "[[0.41141052 0.30109887 0.2869903 ]\n",
      " [0.27609988 0.39435128 0.30012959]\n",
      " [0.30818424 0.30917714 0.41031554]]\n",
      "The estimation error in transition matrix (mse) by dual T estimator is:\n",
      "0.00664760953341222\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by dual T estimator is:')\n",
    "print(best_T_dual_estimated)\n",
    "\n",
    "print(\"The estimation error in transition matrix (mse) by dual T estimator is:\")\n",
    "print((np.square(T_true - best_T_dual_estimated)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWGLDtJJHB9N"
   },
   "source": [
    "## 4.2 FashionMINIST0.6.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU2KPPA5HIyF"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = np.load(\"./data/FashionMNIST0.6.npz\")\n",
    "# Convert its shape\n",
    "Xtr_val = dataset['Xtr'].reshape([18000, 1, 28, 28])\n",
    "Str_val = dataset['Str']\n",
    "# Convert its shape\n",
    "Xts = dataset['Xts'].reshape([3000, 1, 28, 28])\n",
    "Yts = dataset['Yts']\n",
    "\n",
    "# The transition matrix provided\n",
    "Ta = torch.tensor([[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]]).to(device)\n",
    "\n",
    "# Lists to store the testing accuracy\n",
    "test_acc_FNN = []\n",
    "test_acc_CNN = []\n",
    "test_acc_Resnet = []\n",
    "\n",
    "# The optimal model with the bset validation performance\n",
    "best_model = None\n",
    "best_T_estimated = 0\n",
    "best_T_dual_estimated = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "# Loop for 10 random train validation split\n",
    "for i in range(10):\n",
    "    # Random seed\n",
    "    seed_torch(3 ** i)\n",
    "    # Training validation splitting\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Xtr_val, Str_val, test_size=0.2)\n",
    "\n",
    "    # Conver to tensor\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(Xts, dtype=torch.float32)\n",
    "\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test = torch.tensor(Yts, dtype=torch.long)\n",
    "\n",
    "    # For normalization\n",
    "    stats = ((0.1307,), (0.3081,))\n",
    "\n",
    "    # For training transformation\n",
    "    train_tfms = tt.Compose([tt.ToPILImage(),\n",
    "                            tt.RandomHorizontalFlip(), \n",
    "                            tt.ToTensor(), \n",
    "                            tt.Normalize(*stats,inplace=True)])\n",
    "    # For validation and testing transformation   \n",
    "    valid_tfms = tt.Compose([tt.ToPILImage(), tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train, X_val, X_test, y_train, y_val, y_test, train_tfms, valid_tfms, batch_size = 100, num_workers = 0)\n",
    "\n",
    "    # The FNN model\n",
    "    FNN_model = FCNet().to(device)\n",
    "    FNN_optimizer = torch.optim.Adam(FNN_model.parameters(), lr=0.001)\n",
    "    FNN_avg_acc_val, FNN_avg_acc_test, FNN_estimated_T, FNN_estimated_T_dual = train_val_test(FNN_model, train_loader, val_loader, test_loader, FNN_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_FNN.append(float(FNN_avg_acc_test.cpu()))\n",
    "\n",
    "    if FNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = FNN_avg_acc_val\n",
    "        best_model = FNN_model\n",
    "        best_T_estimated = FNN_estimated_T\n",
    "        best_T_dual_estimated = FNN_estimated_T_dual\n",
    "\n",
    "    # The CNN model\n",
    "    CNN_model = CNN(grayscale=True).to(device)\n",
    "    CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "    CNN_avg_acc_val, CNN_avg_acc_test, CNN_estimated_T, CNN_estimated_T_dual = train_val_test(CNN_model, train_loader, val_loader, test_loader, CNN_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_CNN.append(float(CNN_avg_acc_test.cpu()))\n",
    "\n",
    "    if CNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = CNN_avg_acc_val\n",
    "        best_model = CNN_model\n",
    "        best_T_estimated = CNN_estimated_T\n",
    "        best_T_dual_estimated = CNN_estimated_T_dual\n",
    "\n",
    "    # The ResNet model\n",
    "    Resnet_model = ResNet(block=BasicBlock, num_blocks=[2, 2, 2, 2], num_classes=3, grayscale=True).to(device)\n",
    "    Resnet_optimizer = torch.optim.Adam(Resnet_model.parameters(), lr=0.001)\n",
    "    Resnet_avg_acc_val, Resnet_avg_acc_test, Resnet_estimated_T, Resnet_estimated_T_dual = train_val_test(Resnet_model, train_loader, val_loader, test_loader, Resnet_optimizer, Ta, num_epoch = 4)\n",
    "    test_acc_Resnet.append(float(Resnet_avg_acc_test.cpu()))\n",
    "\n",
    "    if Resnet_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = Resnet_avg_acc_val\n",
    "        best_model = Resnet_model\n",
    "        best_T_estimated = Resnet_estimated_T\n",
    "        best_T_dual_estimated = Resnet_estimated_T_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcCKcdpoKnyS",
    "outputId": "88ba149b-8549-40b8-a009-4edcec319a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average testing accuracy FNN is 0.7070, and the std is 0.0308\n",
      "The average testing accuracy CNN is 0.8579, and the std is 0.0278\n",
      "The average testing accuracy ResNet is 0.6950, and the std is 0.0988\n"
     ]
    }
   ],
   "source": [
    "test_acc_FNN = np.array(test_acc_FNN)\n",
    "print(f'The average testing accuracy FNN is {test_acc_FNN.mean():.4f}, and the std is {test_acc_FNN.std():.4f}' )\n",
    "\n",
    "test_acc_CNN = np.array(test_acc_CNN)\n",
    "print(f'The average testing accuracy CNN is {test_acc_CNN.mean():.4f}, and the std is {test_acc_CNN.std():.4f}' )\n",
    "\n",
    "test_acc_Resnet = np.array(test_acc_Resnet)\n",
    "print(f'The average testing accuracy ResNet is {test_acc_Resnet.mean():.4f}, and the std is {test_acc_Resnet.std():.4f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7QQDG-sKnyT",
    "outputId": "c7815c84-fc24-4442-af28-5df378b6d136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by loss correction is:\n",
      "[[0.45818657 0.28933543 0.32200667]\n",
      " [0.26656845 0.43239841 0.29609001]\n",
      " [0.27524495 0.27826613 0.38190329]]\n",
      "The estimation error in transition matrix (mse) by loss correction is:\n",
      "0.000842108992334572\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by loss correction is:')\n",
    "print(best_T_estimated)\n",
    "\n",
    "# The provided transition matrix\n",
    "T_true = np.array([[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]])\n",
    "\n",
    "print(\"The estimation error in transition matrix (mse) by loss correction is:\")\n",
    "print((np.square(T_true - best_T_estimated)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDJzQC9zKnyT",
    "outputId": "f30397dc-8889-4e32-9d9f-df93c720f8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by dual T estimator is:\n",
      "[[0.34532686 0.32872515 0.33246407]\n",
      " [0.32603551 0.34373554 0.32950672]\n",
      " [0.32869206 0.32849888 0.33746317]]\n",
      "The estimation error in transition matrix (mse) by dual T estimator is:\n",
      "0.0016809624247598593\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by dual T estimator is:')\n",
    "print(best_T_dual_estimated)\n",
    "\n",
    "print(\"The estimation error in transition matrix (mse) by dual T estimator is:\")\n",
    "print((np.square(T_true - best_T_dual_estimated)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrnWA4t2HEL9"
   },
   "source": [
    "## 4.3 CIFAR.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkxMeZnJ9qnF"
   },
   "outputs": [],
   "source": [
    "dataset = np.load(\"./data/CIFAR.npz\")\n",
    "Xtr_val = dataset['Xtr']\n",
    "Str_val = dataset['Str']\n",
    "Xts = dataset['Xts']\n",
    "Yts = dataset['Yts']\n",
    "\n",
    "# No transition matrix provided, so we firstly try to estimate it\n",
    "\n",
    "Ta = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]).to(device)\n",
    "\n",
    "# Firstly we need to estimate the transition matrix using 10 random splitting and training. \n",
    "# The model with the best validation performance will be used to estimate the transition matrix\n",
    "\n",
    "# To store the optimal model with the bset validation performance\n",
    "best_model = None\n",
    "best_T_estimated = 0\n",
    "best_T_dual_estimated = 0\n",
    "best_val_acc = 0\n",
    "for i in range(10):\n",
    "    # Random seed\n",
    "    seed_torch(5 * i) \n",
    "\n",
    "    # Random traning validation splitting\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Xtr_val, Str_val, test_size=0.2)\n",
    "\n",
    "    # Convert the dataset to tensor with the correct shape\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).permute(0,3,1,2)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).permute(0,3,1,2)\n",
    "    X_test = torch.tensor(Xts, dtype=torch.float32).permute(0,3,1,2)\n",
    "\n",
    "    # Convert the dataset to tensor\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test = torch.tensor(Yts, dtype=torch.long)\n",
    "\n",
    "    # To nomalize\n",
    "    stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Training transform\n",
    "    train_tfms = tt.Compose([tt.ToPILImage(),\n",
    "                            tt.RandomHorizontalFlip(), \n",
    "                            tt.ToTensor(), \n",
    "                            tt.Normalize(*stats,inplace=True)])\n",
    "    # Validation and testing transform    \n",
    "    valid_tfms = tt.Compose([tt.ToPILImage(), tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train, X_val, X_test, y_train, y_val, y_test, train_tfms, valid_tfms, batch_size = 100, num_workers = 0)\n",
    "\n",
    "    # The FNN model\n",
    "    FNN_model = FCNet(input_dim = 3072).to(device)\n",
    "    FNN_optimizer = torch.optim.Adam(FNN_model.parameters(), lr=0.001)\n",
    "    FNN_avg_acc_val, FNN_avg_acc_test, FNN_estimated_T, FNN_estimated_T_dual = train_val_test(FNN_model, train_loader, val_loader, test_loader, FNN_optimizer, Ta, num_epoch = 4)\n",
    "\n",
    "    # Find the optimal model with highest validation accuracy\n",
    "    if FNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = FNN_avg_acc_val\n",
    "        best_model = FNN_model\n",
    "        best_T_estimated = FNN_estimated_T\n",
    "        best_T_dual_estimated = FNN_estimated_T_dual\n",
    "\n",
    "    # The CNN model\n",
    "    CNN_model = CNN(grayscale=False).to(device)\n",
    "    CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "    CNN_avg_acc_val, CNN_avg_acc_test, CNN_estimated_T, CNN_estimated_T_dual = train_val_test(CNN_model, train_loader, val_loader, test_loader, CNN_optimizer, Ta, num_epoch = 4)\n",
    "\n",
    "    # Find the optimal model with highest validation accuracy\n",
    "    if CNN_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = CNN_avg_acc_val\n",
    "        best_model = CNN_model\n",
    "        best_T_estimated = CNN_estimated_T\n",
    "        best_T_dual_estimated = CNN_estimated_T_dual\n",
    "\n",
    "    # The ResNet model\n",
    "    Resnet_model = ResNet(block=BasicBlock, num_blocks=[2, 2, 2, 2], num_classes=3, grayscale=False).to(device)\n",
    "    Resnet_optimizer = torch.optim.Adam(Resnet_model.parameters(), lr=0.001)\n",
    "    Resnet_avg_acc_val, Resnet_avg_acc_test, Resnet_estimated_T, Resnet_estimated_T_dual = train_val_test(Resnet_model, train_loader, val_loader, test_loader, Resnet_optimizer, Ta, num_epoch = 4)\n",
    "\n",
    "    # Find the optimal model with highest validation accuracy\n",
    "    if Resnet_avg_acc_val > best_val_acc:\n",
    "        best_val_acc = Resnet_avg_acc_val\n",
    "        best_model = Resnet_model\n",
    "        best_T_estimated = Resnet_estimated_T\n",
    "        best_T_dual_estimated = Resnet_estimated_T_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eHYH45F9qnG",
    "outputId": "0e4aab66-a5be-4bd2-ce77-627f82cdff1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by loss correction is:\n",
      "[[0.46960118 0.09578571 0.35432899]\n",
      " [0.3438389  0.5333789  0.21702285]\n",
      " [0.18655995 0.37083539 0.42864814]]\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by loss correction is:')\n",
    "print(best_T_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt0TeRLLKnyT",
    "outputId": "4ed75b83-8a45-4d8c-8141-576858ce3b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimated transition matrix by dual T estimator is:\n",
      "[[0.3493624  0.32104207 0.32735027]\n",
      " [0.32317779 0.35741034 0.3283265 ]\n",
      " [0.32684683 0.31559371 0.3480214 ]]\n"
     ]
    }
   ],
   "source": [
    "print('The best estimated transition matrix by dual T estimator is:')\n",
    "print(best_T_dual_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHPSEUMBKnyU"
   },
   "outputs": [],
   "source": [
    "# After we have estimated the transition matrix by the two methods, we aply the matrix to evaluate their performances\n",
    "dataset = np.load(\"./data/CIFAR.npz\")\n",
    "Xtr_val = dataset['Xtr']\n",
    "Str_val = dataset['Str']\n",
    "Xts = dataset['Xts']\n",
    "Yts = dataset['Yts']\n",
    "\n",
    "\n",
    "# Use the estimated transition matrix\n",
    "Ta = torch.from_numpy(best_T_estimated).float().to(device)\n",
    "Ta_dual = torch.from_numpy(best_T_dual_estimated).float().to(device)\n",
    "\n",
    "\n",
    "# The lists to store the testing accuracy\n",
    "test_acc_FNN = []\n",
    "test_acc_CNN = []\n",
    "test_acc_Resnet = []\n",
    "\n",
    "test_acc_dual_FNN = []\n",
    "test_acc_dual_CNN = []\n",
    "test_acc_dual_Resnet = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Random seed\n",
    "    seed_torch(2 ** i)\n",
    "\n",
    "    # Random training valition splitting\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Xtr_val, Str_val, test_size=0.2)\n",
    "\n",
    "    # Convert the dataset to tensor with the correct shape\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).permute(0,3,1,2)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).permute(0,3,1,2)\n",
    "    X_test = torch.tensor(Xts, dtype=torch.float32).permute(0,3,1,2)\n",
    "\n",
    "    # Convert the dataset to tensor\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test = torch.tensor(Yts, dtype=torch.long)\n",
    "\n",
    "    stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    train_tfms = tt.Compose([tt.ToPILImage(),\n",
    "                            tt.RandomHorizontalFlip(), \n",
    "                            tt.ToTensor(), \n",
    "                            tt.Normalize(*stats,inplace=True)])\n",
    "    \n",
    "    valid_tfms = tt.Compose([tt.ToPILImage(), tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_loader(X_train, X_val, X_test, y_train, y_val, y_test, train_tfms, valid_tfms, batch_size = 100, num_workers = 0)\n",
    "\n",
    "    # The FNN model\n",
    "    FNN_model = FCNet(input_dim = 3072).to(device)\n",
    "    FNN_optimizer = torch.optim.Adam(FNN_model.parameters(), lr=0.001)\n",
    "    FNN_avg_acc_val, FNN_avg_acc_test, FNN_average_acc_dual_test = train_val_test_CIFAR(FNN_model, train_loader, val_loader, test_loader, FNN_optimizer, Ta, Ta_dual, num_epoch = 4)\n",
    "    test_acc_FNN.append(float(FNN_avg_acc_test.cpu()))\n",
    "    test_acc_dual_FNN.append(float(FNN_average_acc_dual_test.cpu()))\n",
    "\n",
    "    # The CNN model\n",
    "    CNN_model = CNN(grayscale=False).to(device)\n",
    "    CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "    CNN_avg_acc_val, CNN_avg_acc_test, CNN_average_acc_dual_test = train_val_test_CIFAR(CNN_model, train_loader, val_loader, test_loader, CNN_optimizer, Ta, Ta_dual, num_epoch = 4)\n",
    "    test_acc_CNN.append(float(CNN_avg_acc_test.cpu()))\n",
    "    test_acc_dual_CNN.append(float(CNN_average_acc_dual_test.cpu()))\n",
    "\n",
    "    # The ResNet model\n",
    "    Resnet_model = ResNet(block=BasicBlock, num_blocks=[2, 2, 2, 2], num_classes=3, grayscale=False).to(device)\n",
    "    Resnet_optimizer = torch.optim.Adam(Resnet_model.parameters(), lr=0.001)\n",
    "    Resnet_avg_acc_val, Resnet_avg_acc_test, Resnet_average_acc_dual_test = train_val_test_CIFAR(Resnet_model, train_loader, val_loader, test_loader, Resnet_optimizer, Ta, Ta_dual, num_epoch = 4)\n",
    "    test_acc_Resnet.append(float(Resnet_avg_acc_test.cpu()))\n",
    "    test_acc_dual_Resnet.append(float(Resnet_average_acc_dual_test.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk8uJzlvKnyU",
    "outputId": "5f04deee-6a5a-4c09-826a-f46018d81cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss correction\n",
      "-------------------------------------------\n",
      "The average testing accuracy FNN is 0.5182, and the std is 0.0191\n",
      "The average testing accuracy CNN is 0.5736, and the std is 0.0951\n",
      "The average testing accuracy ResNet is 0.5207, and the std is 0.0877\n"
     ]
    }
   ],
   "source": [
    "print('loss correction')\n",
    "print('-------------------------------------------')\n",
    "\n",
    "test_acc_dual_FNN = np.array(test_acc_dual_FNN)\n",
    "print(f'The average testing accuracy FNN is {test_acc_dual_FNN.mean():.4f}, and the std is {test_acc_dual_FNN.std():.4f}' )\n",
    "\n",
    "test_acc_dual_CNN = np.array(test_acc_dual_CNN)\n",
    "print(f'The average testing accuracy CNN is {test_acc_dual_CNN.mean():.4f}, and the std is {test_acc_dual_CNN.std():.4f}' )\n",
    "\n",
    "test_acc_Resnet = np.array(test_acc_Resnet)\n",
    "print(f'The average testing accuracy ResNet is {test_acc_Resnet.mean():.4f}, and the std is {test_acc_Resnet.std():.4f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5i5jOWOKnyU",
    "outputId": "d93e6d7d-b2d8-44ff-b5db-eb132b0ad4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dual T estimator\n",
      "-------------------------------------------\n",
      "The average testing accuracy FNN is 0.5435, and the std is 0.0282\n",
      "The average testing accuracy CNN is 0.6384, and the std is 0.0931\n",
      "The average testing accuracy ResNet is 0.5053, and the std is 0.0666\n"
     ]
    }
   ],
   "source": [
    "print('dual T estimator')\n",
    "print('-------------------------------------------')\n",
    "\n",
    "test_acc_FNN = np.array(test_acc_FNN)\n",
    "print(f'The average testing accuracy FNN is {test_acc_FNN.mean():.4f}, and the std is {test_acc_FNN.std():.4f}' )\n",
    "\n",
    "test_acc_CNN = np.array(test_acc_CNN)\n",
    "print(f'The average testing accuracy CNN is {test_acc_CNN.mean():.4f}, and the std is {test_acc_CNN.std():.4f}' )\n",
    "\n",
    "test_acc_dual_Resnet = np.array(test_acc_dual_Resnet)\n",
    "print(f'The average testing accuracy ResNet is {test_acc_dual_Resnet.mean():.4f}, and the std is {test_acc_dual_Resnet.std():.4f}' )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "0814ccf4d854647ca2b3a3d8eac83034772b1e2507561d194f7f619ef0b1500f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
